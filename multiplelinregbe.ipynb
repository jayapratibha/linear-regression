{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport sklearn ","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install statsmodels","execution_count":17,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: statsmodels in /srv/conda/envs/notebook/lib/python3.7/site-packages (0.11.1)\nRequirement already satisfied: patsy>=0.5 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from statsmodels) (0.5.1)\nRequirement already satisfied: numpy>=1.14 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from statsmodels) (1.19.1)\nRequirement already satisfied: pandas>=0.21 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from statsmodels) (0.24.2)\nRequirement already satisfied: scipy>=1.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from statsmodels) (1.5.2)\nRequirement already satisfied: six in /srv/conda/envs/notebook/lib/python3.7/site-packages (from patsy>=0.5->statsmodels) (1.15.0)\nRequirement already satisfied: pytz>=2011k in /srv/conda/envs/notebook/lib/python3.7/site-packages (from pandas>=0.21->statsmodels) (2020.1)\nRequirement already satisfied: python-dateutil>=2.5.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from pandas>=0.21->statsmodels) (2.8.1)\nNote: you may need to restart the kernel to use updated packages.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.formula.api as sm","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing the dataset\ndataset = pd.read_csv('50_Startups.csv')\nX = dataset.iloc[:,:-1].values \ny = dataset.iloc[:,4].values","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# encoding categorical data\n# encoding the independent varaible ie the label\n\nfrom sklearn.preprocessing import LabelEncoder , OneHotEncoder\nlabelencoder_X = LabelEncoder()\n(X[:,3])=labelencoder_X.fit_transform(X[:,3])","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\n\nct = ColumnTransformer(\n    [('one_hot_encoder', OneHotEncoder(categories='auto'), [3])],   # The column numbers to be transformed (here is [0] but can be [0, 1, 3])\n    remainder='passthrough'                                         # Leave the rest of the columns untouched\n)\n\nX = ct.fit_transform(X)","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X","execution_count":24,"outputs":[{"output_type":"execute_result","execution_count":24,"data":{"text/plain":"array([[1, 0.0, 0.0, 1.0, 165349.2, 136897.8, 471784.1],\n       [1, 1.0, 0.0, 0.0, 162597.7, 151377.59, 443898.53],\n       [1, 0.0, 1.0, 0.0, 153441.51, 101145.55, 407934.54],\n       [1, 0.0, 0.0, 1.0, 144372.41, 118671.85, 383199.62],\n       [1, 0.0, 1.0, 0.0, 142107.34, 91391.77, 366168.42],\n       [1, 0.0, 0.0, 1.0, 131876.9, 99814.71, 362861.36],\n       [1, 1.0, 0.0, 0.0, 134615.46, 147198.87, 127716.82],\n       [1, 0.0, 1.0, 0.0, 130298.13, 145530.06, 323876.68],\n       [1, 0.0, 0.0, 1.0, 120542.52, 148718.95, 311613.29],\n       [1, 1.0, 0.0, 0.0, 123334.88, 108679.17, 304981.62],\n       [1, 0.0, 1.0, 0.0, 101913.08, 110594.11, 229160.95],\n       [1, 1.0, 0.0, 0.0, 100671.96, 91790.61, 249744.55],\n       [1, 0.0, 1.0, 0.0, 93863.75, 127320.38, 249839.44],\n       [1, 1.0, 0.0, 0.0, 91992.39, 135495.07, 252664.93],\n       [1, 0.0, 1.0, 0.0, 119943.24, 156547.42, 256512.92],\n       [1, 0.0, 0.0, 1.0, 114523.61, 122616.84, 261776.23],\n       [1, 1.0, 0.0, 0.0, 78013.11, 121597.55, 264346.06],\n       [1, 0.0, 0.0, 1.0, 94657.16, 145077.58, 282574.31],\n       [1, 0.0, 1.0, 0.0, 91749.16, 114175.79, 294919.57],\n       [1, 0.0, 0.0, 1.0, 86419.7, 153514.11, 0.0],\n       [1, 1.0, 0.0, 0.0, 76253.86, 113867.3, 298664.47],\n       [1, 0.0, 0.0, 1.0, 78389.47, 153773.43, 299737.29],\n       [1, 0.0, 1.0, 0.0, 73994.56, 122782.75, 303319.26],\n       [1, 0.0, 1.0, 0.0, 67532.53, 105751.03, 304768.73],\n       [1, 0.0, 0.0, 1.0, 77044.01, 99281.34, 140574.81],\n       [1, 1.0, 0.0, 0.0, 64664.71, 139553.16, 137962.62],\n       [1, 0.0, 1.0, 0.0, 75328.87, 144135.98, 134050.07],\n       [1, 0.0, 0.0, 1.0, 72107.6, 127864.55, 353183.81],\n       [1, 0.0, 1.0, 0.0, 66051.52, 182645.56, 118148.2],\n       [1, 0.0, 0.0, 1.0, 65605.48, 153032.06, 107138.38],\n       [1, 0.0, 1.0, 0.0, 61994.48, 115641.28, 91131.24],\n       [1, 0.0, 0.0, 1.0, 61136.38, 152701.92, 88218.23],\n       [1, 1.0, 0.0, 0.0, 63408.86, 129219.61, 46085.25],\n       [1, 0.0, 1.0, 0.0, 55493.95, 103057.49, 214634.81],\n       [1, 1.0, 0.0, 0.0, 46426.07, 157693.92, 210797.67],\n       [1, 0.0, 0.0, 1.0, 46014.02, 85047.44, 205517.64],\n       [1, 0.0, 1.0, 0.0, 28663.76, 127056.21, 201126.82],\n       [1, 1.0, 0.0, 0.0, 44069.95, 51283.14, 197029.42],\n       [1, 0.0, 0.0, 1.0, 20229.59, 65947.93, 185265.1],\n       [1, 1.0, 0.0, 0.0, 38558.51, 82982.09, 174999.3],\n       [1, 1.0, 0.0, 0.0, 28754.33, 118546.05, 172795.67],\n       [1, 0.0, 1.0, 0.0, 27892.92, 84710.77, 164470.71],\n       [1, 1.0, 0.0, 0.0, 23640.93, 96189.63, 148001.11],\n       [1, 0.0, 0.0, 1.0, 15505.73, 127382.3, 35534.17],\n       [1, 1.0, 0.0, 0.0, 22177.74, 154806.14, 28334.72],\n       [1, 0.0, 0.0, 1.0, 1000.23, 124153.04, 1903.93],\n       [1, 0.0, 1.0, 0.0, 1315.46, 115816.21, 297114.46],\n       [1, 1.0, 0.0, 0.0, 0.0, 135426.92, 0.0],\n       [1, 0.0, 0.0, 1.0, 542.05, 51743.15, 0.0],\n       [1, 1.0, 0.0, 0.0, 0.0, 116983.8, 45173.06]], dtype=object)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2 , random_state = 0)","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train","execution_count":27,"outputs":[{"output_type":"execute_result","execution_count":27,"data":{"text/plain":"array([[1, 0.0, 1.0, 0.0, 55493.95, 103057.49, 214634.81],\n       [1, 0.0, 0.0, 1.0, 46014.02, 85047.44, 205517.64],\n       [1, 0.0, 1.0, 0.0, 75328.87, 144135.98, 134050.07],\n       [1, 1.0, 0.0, 0.0, 46426.07, 157693.92, 210797.67],\n       [1, 0.0, 1.0, 0.0, 91749.16, 114175.79, 294919.57],\n       [1, 0.0, 1.0, 0.0, 130298.13, 145530.06, 323876.68],\n       [1, 0.0, 1.0, 0.0, 119943.24, 156547.42, 256512.92],\n       [1, 0.0, 0.0, 1.0, 1000.23, 124153.04, 1903.93],\n       [1, 0.0, 0.0, 1.0, 542.05, 51743.15, 0.0],\n       [1, 0.0, 0.0, 1.0, 65605.48, 153032.06, 107138.38],\n       [1, 0.0, 0.0, 1.0, 114523.61, 122616.84, 261776.23],\n       [1, 0.0, 1.0, 0.0, 61994.48, 115641.28, 91131.24],\n       [1, 1.0, 0.0, 0.0, 63408.86, 129219.61, 46085.25],\n       [1, 1.0, 0.0, 0.0, 78013.11, 121597.55, 264346.06],\n       [1, 1.0, 0.0, 0.0, 23640.93, 96189.63, 148001.11],\n       [1, 1.0, 0.0, 0.0, 76253.86, 113867.3, 298664.47],\n       [1, 0.0, 0.0, 1.0, 15505.73, 127382.3, 35534.17],\n       [1, 0.0, 0.0, 1.0, 120542.52, 148718.95, 311613.29],\n       [1, 1.0, 0.0, 0.0, 91992.39, 135495.07, 252664.93],\n       [1, 1.0, 0.0, 0.0, 64664.71, 139553.16, 137962.62],\n       [1, 0.0, 0.0, 1.0, 131876.9, 99814.71, 362861.36],\n       [1, 0.0, 0.0, 1.0, 94657.16, 145077.58, 282574.31],\n       [1, 1.0, 0.0, 0.0, 28754.33, 118546.05, 172795.67],\n       [1, 1.0, 0.0, 0.0, 0.0, 116983.8, 45173.06],\n       [1, 1.0, 0.0, 0.0, 162597.7, 151377.59, 443898.53],\n       [1, 0.0, 1.0, 0.0, 93863.75, 127320.38, 249839.44],\n       [1, 1.0, 0.0, 0.0, 44069.95, 51283.14, 197029.42],\n       [1, 0.0, 0.0, 1.0, 77044.01, 99281.34, 140574.81],\n       [1, 1.0, 0.0, 0.0, 134615.46, 147198.87, 127716.82],\n       [1, 0.0, 1.0, 0.0, 67532.53, 105751.03, 304768.73],\n       [1, 0.0, 1.0, 0.0, 28663.76, 127056.21, 201126.82],\n       [1, 0.0, 0.0, 1.0, 78389.47, 153773.43, 299737.29],\n       [1, 0.0, 0.0, 1.0, 86419.7, 153514.11, 0.0],\n       [1, 1.0, 0.0, 0.0, 123334.88, 108679.17, 304981.62],\n       [1, 1.0, 0.0, 0.0, 38558.51, 82982.09, 174999.3],\n       [1, 0.0, 1.0, 0.0, 1315.46, 115816.21, 297114.46],\n       [1, 0.0, 0.0, 1.0, 144372.41, 118671.85, 383199.62],\n       [1, 0.0, 0.0, 1.0, 165349.2, 136897.8, 471784.1],\n       [1, 1.0, 0.0, 0.0, 0.0, 135426.92, 0.0],\n       [1, 1.0, 0.0, 0.0, 22177.74, 154806.14, 28334.72]], dtype=object)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test","execution_count":29,"outputs":[{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"array([[1, 0.0, 1.0, 0.0, 66051.52, 182645.56, 118148.2],\n       [1, 1.0, 0.0, 0.0, 100671.96, 91790.61, 249744.55],\n       [1, 0.0, 1.0, 0.0, 101913.08, 110594.11, 229160.95],\n       [1, 0.0, 1.0, 0.0, 27892.92, 84710.77, 164470.71],\n       [1, 0.0, 1.0, 0.0, 153441.51, 101145.55, 407934.54],\n       [1, 0.0, 0.0, 1.0, 72107.6, 127864.55, 353183.81],\n       [1, 0.0, 0.0, 1.0, 20229.59, 65947.93, 185265.1],\n       [1, 0.0, 0.0, 1.0, 61136.38, 152701.92, 88218.23],\n       [1, 0.0, 1.0, 0.0, 73994.56, 122782.75, 303319.26],\n       [1, 0.0, 1.0, 0.0, 142107.34, 91391.77, 366168.42]], dtype=object)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train","execution_count":30,"outputs":[{"output_type":"execute_result","execution_count":30,"data":{"text/plain":"array([ 96778.92,  96479.51, 105733.54,  96712.8 , 124266.9 , 155752.6 ,\n       132602.65,  64926.08,  35673.41, 101004.64, 129917.04,  99937.59,\n        97427.84, 126992.93,  71498.49, 118474.03,  69758.98, 152211.77,\n       134307.35, 107404.34, 156991.12, 125370.37,  78239.91,  14681.4 ,\n       191792.06, 141585.52,  89949.14, 108552.04, 156122.51, 108733.99,\n        90708.19, 111313.02, 122776.86, 149759.96,  81005.76,  49490.75,\n       182901.99, 192261.83,  42559.73,  65200.33])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test","execution_count":31,"outputs":[{"output_type":"execute_result","execution_count":31,"data":{"text/plain":"array([103282.38, 144259.4 , 146121.95,  77798.83, 191050.39, 105008.31,\n        81229.06,  97483.56, 110352.25, 166187.94])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nimport statsmodels.regression.linear_model as lm\nregressor = LinearRegression()\nregressor.fit(X_train,y_train)","execution_count":65,"outputs":[{"output_type":"execute_result","execution_count":65,"data":{"text/plain":"LinearRegression()"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# predicting the tests set results\ny_pred = regressor.predict(X_test)","execution_count":53,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred","execution_count":54,"outputs":[{"output_type":"execute_result","execution_count":54,"data":{"text/plain":"array([103015.20159794, 132582.27760816, 132447.73845175,  71976.09851259,\n       178537.48221058, 116161.24230167,  67851.69209677,  98791.73374686,\n       113969.43533014, 167921.06569553])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test","execution_count":55,"outputs":[{"output_type":"execute_result","execution_count":55,"data":{"text/plain":"array([103282.38, 144259.4 , 146121.95,  77798.83, 191050.39, 105008.31,\n        81229.06,  97483.56, 110352.25, 166187.94])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# compare real and predicted profit","execution_count":56,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.append(np.ones((50,1)).astype(int) ,values=X , axis=1)","execution_count":68,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# below we create a matrix containing only high impact independent varaibles so remove one by one","execution_count":69,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_opt = X[:,[0,1,2,3,4,5]]","execution_count":76,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_opt","execution_count":77,"outputs":[{"output_type":"execute_result","execution_count":77,"data":{"text/plain":"array([[1, 1, 1, 1, 0.0, 0.0],\n       [1, 1, 1, 1, 1.0, 0.0],\n       [1, 1, 1, 1, 0.0, 1.0],\n       [1, 1, 1, 1, 0.0, 0.0],\n       [1, 1, 1, 1, 0.0, 1.0],\n       [1, 1, 1, 1, 0.0, 0.0],\n       [1, 1, 1, 1, 1.0, 0.0],\n       [1, 1, 1, 1, 0.0, 1.0],\n       [1, 1, 1, 1, 0.0, 0.0],\n       [1, 1, 1, 1, 1.0, 0.0],\n       [1, 1, 1, 1, 0.0, 1.0],\n       [1, 1, 1, 1, 1.0, 0.0],\n       [1, 1, 1, 1, 0.0, 1.0],\n       [1, 1, 1, 1, 1.0, 0.0],\n       [1, 1, 1, 1, 0.0, 1.0],\n       [1, 1, 1, 1, 0.0, 0.0],\n       [1, 1, 1, 1, 1.0, 0.0],\n       [1, 1, 1, 1, 0.0, 0.0],\n       [1, 1, 1, 1, 0.0, 1.0],\n       [1, 1, 1, 1, 0.0, 0.0],\n       [1, 1, 1, 1, 1.0, 0.0],\n       [1, 1, 1, 1, 0.0, 0.0],\n       [1, 1, 1, 1, 0.0, 1.0],\n       [1, 1, 1, 1, 0.0, 1.0],\n       [1, 1, 1, 1, 0.0, 0.0],\n       [1, 1, 1, 1, 1.0, 0.0],\n       [1, 1, 1, 1, 0.0, 1.0],\n       [1, 1, 1, 1, 0.0, 0.0],\n       [1, 1, 1, 1, 0.0, 1.0],\n       [1, 1, 1, 1, 0.0, 0.0],\n       [1, 1, 1, 1, 0.0, 1.0],\n       [1, 1, 1, 1, 0.0, 0.0],\n       [1, 1, 1, 1, 1.0, 0.0],\n       [1, 1, 1, 1, 0.0, 1.0],\n       [1, 1, 1, 1, 1.0, 0.0],\n       [1, 1, 1, 1, 0.0, 0.0],\n       [1, 1, 1, 1, 0.0, 1.0],\n       [1, 1, 1, 1, 1.0, 0.0],\n       [1, 1, 1, 1, 0.0, 0.0],\n       [1, 1, 1, 1, 1.0, 0.0],\n       [1, 1, 1, 1, 1.0, 0.0],\n       [1, 1, 1, 1, 0.0, 1.0],\n       [1, 1, 1, 1, 1.0, 0.0],\n       [1, 1, 1, 1, 0.0, 0.0],\n       [1, 1, 1, 1, 1.0, 0.0],\n       [1, 1, 1, 1, 0.0, 0.0],\n       [1, 1, 1, 1, 0.0, 1.0],\n       [1, 1, 1, 1, 1.0, 0.0],\n       [1, 1, 1, 1, 0.0, 0.0],\n       [1, 1, 1, 1, 1.0, 0.0]], dtype=object)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_opt = np.array(X_opt, dtype=float)","execution_count":78,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_opt","execution_count":79,"outputs":[{"output_type":"execute_result","execution_count":79,"data":{"text/plain":"array([[1., 1., 1., 1., 0., 0.],\n       [1., 1., 1., 1., 1., 0.],\n       [1., 1., 1., 1., 0., 1.],\n       [1., 1., 1., 1., 0., 0.],\n       [1., 1., 1., 1., 0., 1.],\n       [1., 1., 1., 1., 0., 0.],\n       [1., 1., 1., 1., 1., 0.],\n       [1., 1., 1., 1., 0., 1.],\n       [1., 1., 1., 1., 0., 0.],\n       [1., 1., 1., 1., 1., 0.],\n       [1., 1., 1., 1., 0., 1.],\n       [1., 1., 1., 1., 1., 0.],\n       [1., 1., 1., 1., 0., 1.],\n       [1., 1., 1., 1., 1., 0.],\n       [1., 1., 1., 1., 0., 1.],\n       [1., 1., 1., 1., 0., 0.],\n       [1., 1., 1., 1., 1., 0.],\n       [1., 1., 1., 1., 0., 0.],\n       [1., 1., 1., 1., 0., 1.],\n       [1., 1., 1., 1., 0., 0.],\n       [1., 1., 1., 1., 1., 0.],\n       [1., 1., 1., 1., 0., 0.],\n       [1., 1., 1., 1., 0., 1.],\n       [1., 1., 1., 1., 0., 1.],\n       [1., 1., 1., 1., 0., 0.],\n       [1., 1., 1., 1., 1., 0.],\n       [1., 1., 1., 1., 0., 1.],\n       [1., 1., 1., 1., 0., 0.],\n       [1., 1., 1., 1., 0., 1.],\n       [1., 1., 1., 1., 0., 0.],\n       [1., 1., 1., 1., 0., 1.],\n       [1., 1., 1., 1., 0., 0.],\n       [1., 1., 1., 1., 1., 0.],\n       [1., 1., 1., 1., 0., 1.],\n       [1., 1., 1., 1., 1., 0.],\n       [1., 1., 1., 1., 0., 0.],\n       [1., 1., 1., 1., 0., 1.],\n       [1., 1., 1., 1., 1., 0.],\n       [1., 1., 1., 1., 0., 0.],\n       [1., 1., 1., 1., 1., 0.],\n       [1., 1., 1., 1., 1., 0.],\n       [1., 1., 1., 1., 0., 1.],\n       [1., 1., 1., 1., 1., 0.],\n       [1., 1., 1., 1., 0., 0.],\n       [1., 1., 1., 1., 1., 0.],\n       [1., 1., 1., 1., 0., 0.],\n       [1., 1., 1., 1., 0., 1.],\n       [1., 1., 1., 1., 1., 0.],\n       [1., 1., 1., 1., 0., 0.],\n       [1., 1., 1., 1., 1., 0.]])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nregressor_ols = lm.OLS(endog = y,exog = X_opt).fit()","execution_count":74,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm","execution_count":80,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# above we have fit the model with all possile predictors","execution_count":81,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor_ols.summary()","execution_count":82,"outputs":[{"output_type":"execute_result","execution_count":82,"data":{"text/plain":"<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.024\nModel:                            OLS   Adj. R-squared:                 -0.018\nMethod:                 Least Squares   F-statistic:                    0.5748\nDate:                Mon, 17 Aug 2020   Prob (F-statistic):              0.567\nTime:                        04:24:47   Log-Likelihood:                -600.05\nNo. Observations:                  50   AIC:                             1206.\nDf Residuals:                      47   BIC:                             1212.\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst       2.844e+04   2465.409     11.535      0.000    2.35e+04    3.34e+04\nx1          2.844e+04   2465.409     11.535      0.000    2.35e+04    3.34e+04\nx2          2.844e+04   2465.409     11.535      0.000    2.35e+04    3.34e+04\nx3          2.844e+04   2465.409     11.535      0.000    2.35e+04    3.34e+04\nx4         -9851.2712   1.39e+04     -0.706      0.483   -3.79e+04    1.82e+04\nx5          5017.5779   1.42e+04      0.354      0.725   -2.35e+04    3.35e+04\n==============================================================================\nOmnibus:                        0.111   Durbin-Watson:                   0.081\nProb(Omnibus):                  0.946   Jarque-Bera (JB):                0.207\nSkew:                           0.104   Prob(JB):                        0.902\nKurtosis:                       2.762   Cond. No.                     6.54e+18\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The smallest eigenvalue is 4.93e-36. This might indicate that there are\nstrong multicollinearity problems or that the design matrix is singular.\n\"\"\"","text/html":"<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.024</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>  -0.018</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>  0.5748</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Mon, 17 Aug 2020</td> <th>  Prob (F-statistic):</th>  <td> 0.567</td> \n</tr>\n<tr>\n  <th>Time:</th>                 <td>04:24:47</td>     <th>  Log-Likelihood:    </th> <td> -600.05</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1206.</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>    47</td>      <th>  BIC:               </th> <td>   1212.</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th> <td> 2.844e+04</td> <td> 2465.409</td> <td>   11.535</td> <td> 0.000</td> <td> 2.35e+04</td> <td> 3.34e+04</td>\n</tr>\n<tr>\n  <th>x1</th>    <td> 2.844e+04</td> <td> 2465.409</td> <td>   11.535</td> <td> 0.000</td> <td> 2.35e+04</td> <td> 3.34e+04</td>\n</tr>\n<tr>\n  <th>x2</th>    <td> 2.844e+04</td> <td> 2465.409</td> <td>   11.535</td> <td> 0.000</td> <td> 2.35e+04</td> <td> 3.34e+04</td>\n</tr>\n<tr>\n  <th>x3</th>    <td> 2.844e+04</td> <td> 2465.409</td> <td>   11.535</td> <td> 0.000</td> <td> 2.35e+04</td> <td> 3.34e+04</td>\n</tr>\n<tr>\n  <th>x4</th>    <td>-9851.2712</td> <td> 1.39e+04</td> <td>   -0.706</td> <td> 0.483</td> <td>-3.79e+04</td> <td> 1.82e+04</td>\n</tr>\n<tr>\n  <th>x5</th>    <td> 5017.5779</td> <td> 1.42e+04</td> <td>    0.354</td> <td> 0.725</td> <td>-2.35e+04</td> <td> 3.35e+04</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td> 0.111</td> <th>  Durbin-Watson:     </th> <td>   0.081</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.946</td> <th>  Jarque-Bera (JB):  </th> <td>   0.207</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td> 0.104</td> <th>  Prob(JB):          </th> <td>   0.902</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 2.762</td> <th>  Cond. No.          </th> <td>6.54e+18</td>\n</tr>\n</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 4.93e-36. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# above see p values the lower the p valuse the more significance it will be select highest and remove it varaibles","execution_count":84,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_opt = X[:,[0,1,2,3,4,5]]\nX_opt = np.array(X_opt, dtype=float)\nregressor_ols = lm.OLS(endog = y,exog = X_opt).fit()\nregressor_ols.summary()","execution_count":86,"outputs":[{"output_type":"execute_result","execution_count":86,"data":{"text/plain":"<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.024\nModel:                            OLS   Adj. R-squared:                 -0.018\nMethod:                 Least Squares   F-statistic:                    0.5748\nDate:                Mon, 17 Aug 2020   Prob (F-statistic):              0.567\nTime:                        04:31:03   Log-Likelihood:                -600.05\nNo. Observations:                  50   AIC:                             1206.\nDf Residuals:                      47   BIC:                             1212.\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst       2.844e+04   2465.409     11.535      0.000    2.35e+04    3.34e+04\nx1          2.844e+04   2465.409     11.535      0.000    2.35e+04    3.34e+04\nx2          2.844e+04   2465.409     11.535      0.000    2.35e+04    3.34e+04\nx3          2.844e+04   2465.409     11.535      0.000    2.35e+04    3.34e+04\nx4         -9851.2712   1.39e+04     -0.706      0.483   -3.79e+04    1.82e+04\nx5          5017.5779   1.42e+04      0.354      0.725   -2.35e+04    3.35e+04\n==============================================================================\nOmnibus:                        0.111   Durbin-Watson:                   0.081\nProb(Omnibus):                  0.946   Jarque-Bera (JB):                0.207\nSkew:                           0.104   Prob(JB):                        0.902\nKurtosis:                       2.762   Cond. No.                     6.54e+18\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The smallest eigenvalue is 4.93e-36. This might indicate that there are\nstrong multicollinearity problems or that the design matrix is singular.\n\"\"\"","text/html":"<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.024</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>  -0.018</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>  0.5748</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Mon, 17 Aug 2020</td> <th>  Prob (F-statistic):</th>  <td> 0.567</td> \n</tr>\n<tr>\n  <th>Time:</th>                 <td>04:31:03</td>     <th>  Log-Likelihood:    </th> <td> -600.05</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1206.</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>    47</td>      <th>  BIC:               </th> <td>   1212.</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th> <td> 2.844e+04</td> <td> 2465.409</td> <td>   11.535</td> <td> 0.000</td> <td> 2.35e+04</td> <td> 3.34e+04</td>\n</tr>\n<tr>\n  <th>x1</th>    <td> 2.844e+04</td> <td> 2465.409</td> <td>   11.535</td> <td> 0.000</td> <td> 2.35e+04</td> <td> 3.34e+04</td>\n</tr>\n<tr>\n  <th>x2</th>    <td> 2.844e+04</td> <td> 2465.409</td> <td>   11.535</td> <td> 0.000</td> <td> 2.35e+04</td> <td> 3.34e+04</td>\n</tr>\n<tr>\n  <th>x3</th>    <td> 2.844e+04</td> <td> 2465.409</td> <td>   11.535</td> <td> 0.000</td> <td> 2.35e+04</td> <td> 3.34e+04</td>\n</tr>\n<tr>\n  <th>x4</th>    <td>-9851.2712</td> <td> 1.39e+04</td> <td>   -0.706</td> <td> 0.483</td> <td>-3.79e+04</td> <td> 1.82e+04</td>\n</tr>\n<tr>\n  <th>x5</th>    <td> 5017.5779</td> <td> 1.42e+04</td> <td>    0.354</td> <td> 0.725</td> <td>-2.35e+04</td> <td> 3.35e+04</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td> 0.111</td> <th>  Durbin-Watson:     </th> <td>   0.081</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.946</td> <th>  Jarque-Bera (JB):  </th> <td>   0.207</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td> 0.104</td> <th>  Prob(JB):          </th> <td>   0.902</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 2.762</td> <th>  Cond. No.          </th> <td>6.54e+18</td>\n</tr>\n</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 4.93e-36. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}